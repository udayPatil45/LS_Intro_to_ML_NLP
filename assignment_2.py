# -*- coding: utf-8 -*-
"""Assignment_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oYqHgYLgdkzQerSjGUBEkuO7_TYMwh8W

# Assignment 2

# Problem 1
"""

import pandas as pd
import string
import re
import inflect
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize

df = pd.read_csv('spam.csv', encoding='latin-1')
df.head(5)

df = df.rename(columns={'v1': 'Label', 'v2': 'text'})
df = df.drop(columns=['Unnamed: 2','Unnamed: 3', 'Unnamed: 4'])
df.head(5)

"""Text Processing"""

# defining some functions
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()  # lowercase
    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation
    text = re.sub(r'\d+', '', text)  # remove numbers
    text = text.strip()  # remove leading/trailing whitespace
    return text

def remove_stopwords(text):
    stop_words = set(stopwords.words("english"))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word not in stop_words]
    return filtered_text

def stem_words(text):
    word_tokens = word_tokenize(text)
    stems = [stemmer.stem(word) for word in word_tokens]
    return stems

def lemma_words(text):
    word_tokens = word_tokenize(text)
    lemmas = [lemmatizer.lemmatize(word) for word in word_tokens]
    return lemmas

# now apply function to text
df1 = df.copy()
df1['text'] = df1['text'].apply(clean_text)
df1['text'] = df1['text'].apply(remove_stopwords)
df1['text'] = df1['text'].apply(lambda x: ' '.join(x)) # Join the list back into a string
df1['text'] = df1['text'].apply(stem_words)
df1.head(5)

import gensim.downloader as api
w2v_model = api.load("glove-wiki-gigaword-100")  # 100-dimensional

import numpy as np
def get_vector(words):
    word_vecs = [w2v_model[word] for word in words if word in w2v_model]
    if len(word_vecs) == 0:
        return np.zeros(100)
    return np.mean(word_vecs, axis=0)

df1['vector'] = df1['text'].apply(get_vector)
df1.head()

from sklearn.model_selection import train_test_split

X = np.vstack(df1['vector'].values)
y = df1['Label'].map({'ham': 0, 'spam': 1})

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))

"""# Problem 2"""

!pip install gensim
!pip install contractions

from gensim.models import KeyedVectors
import contractions
contractions.fix("don't")

# Load the CSV
df2 = pd.read_csv('Tweets.csv')
df2.head(1)

# Keeping only the needed columns
df2 = df2[['airline_sentiment', 'text']]
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_tweet(tweet):
    tweet = tweet.lower()
    tweet = contractions.fix(tweet)  # expand contractions
    tweet = re.sub(r'http\S+|www\S+|https\S+', '', tweet)  # remove URLs
    tweet = re.sub(r'@\w+', '', tweet)  # remove mentions
    tweet = re.sub(r'#\w+', '', tweet)  # remove hashtags
    tweet = re.sub(r'[^a-z\s]', '', tweet)  # remove punctuation & special chars
    tokens = word_tokenize(tweet)
    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]
    return tokens

df2['tokens'] = df2['text'].apply(clean_tweet)

df2.head()

# STEP 1: Ensure correct shape in get_avg_vector

def get_avg_vector(tokens, model, dim=100):
    vectors = [model[word] for word in tokens if word in model]
    return np.mean(vectors, axis=0) if vectors else np.zeros(dim)

df2['vector'] = df2['tokens'].apply(lambda tokens: get_avg_vector(tokens, w2v_model))

# STEP 3: Now safe to stack
X = np.vstack(df2['vector'].values)
y = df2['airline_sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test set accuracy:", accuracy)

def predict_tweet_sentiment(model, w2v_model, tweet):
    tokens = clean_tweet(tweet)
    vector = get_avg_vector(tokens, w2v_model).reshape(1, -1)
    return model.predict(vector)[0]

tweet = "I love flying with JetBlue! Always on time and friendly staff."
print("Predicted sentiment:", predict_tweet_sentiment(model, w2v_model, tweet))